{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref https://github.com/AiswaryaSrinivas/DataScienceWithPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint, time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import scorers\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentence = nltk.corpus.treebank.tagged_sents(tagset='universal')\n",
    "tagged_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tagged Sentences 3914\n",
      "Total Number of Tagged Words 100676\n",
      "Vocabulary of the Corpus 12408\n",
      "Number of Tags in the Corpus 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Tagged Sentences\", len(tagged_sentence))\n",
    "tagged_words = [tup for sent in tagged_sentence for tup in sent]\n",
    "print(\"Total Number of Tagged Words\", len(tagged_words))\n",
    "vocab = set([word for word, tag in tagged_words])\n",
    "print(\"Vocabulary of the Corpus\", len(vocab))\n",
    "tags = set([tag for word, tag in tagged_words])\n",
    "print(\"Number of Tags in the Corpus\", len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences in Training Data  3131\n",
      "Number of Sentences in Testing Data  783\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = train_test_split(tagged_sentence, test_size=0.2, random_state=1234)\n",
    "print(\"Number of Sentences in Training Data \", len(train_set))\n",
    "print(\"Number of Sentences in Testing Data \", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "  return {\n",
    "      'is_first_capital' : int(sentence[index][0].isupper()),\n",
    "      'is_first_word' : int(index==0),\n",
    "      'is_last_word' : int(index==len(sentence)-1),\n",
    "      'is_complete_capital' : int(sentence[index].upper()==sentence[index]),\n",
    "      'prev_word' : '' if index==0 else sentence[index-1],\n",
    "      'next_word' : '' if index==len(sentence)-1 else sentence[index+1],\n",
    "      'is_numeric' : int(sentence[index].isdigit()),\n",
    "      'is_alphanumeric' : int(bool((re.match(\"^(?=.*[0-9]$)(?=.*[a-zA-Z])\", sentence[index])))),\n",
    "      'prefix_1' : sentence[index][0],\n",
    "      'prefix_2' : sentence[index][:2],\n",
    "      'prefix_3' : sentence[index][:3],\n",
    "      'prefix_4' : sentence[index][:4],\n",
    "      'suffix_1' : sentence[index][-1],\n",
    "      'suffix_2' : sentence[index][-2:],\n",
    "      'suffix_3' : sentence[index][-3:],\n",
    "      'suffix_4' : sentence[index][-4:],\n",
    "      'word_has_hyphen' : 1 if '-' in sentence[index] else 0\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untag(sentence):\n",
    "    return  [word for word, tag in sentence]\n",
    "\n",
    "def prepareData(tagged_sentences):\n",
    "    X, y = [], []\n",
    "    for sentences in tagged_sentences:\n",
    "        X.append([features(untag(sentences), index) for index in range(len(sentences))])\n",
    "        y.append([tag for word, tag in sentences])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_first_capital': 1,\n",
       "  'is_first_word': 1,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': '',\n",
       "  'next_word': 'Wall',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'O',\n",
       "  'prefix_2': 'On',\n",
       "  'prefix_3': 'On',\n",
       "  'prefix_4': 'On',\n",
       "  'suffix_1': 'n',\n",
       "  'suffix_2': 'On',\n",
       "  'suffix_3': 'On',\n",
       "  'suffix_4': 'On',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 1,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'On',\n",
       "  'next_word': 'Street',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'W',\n",
       "  'prefix_2': 'Wa',\n",
       "  'prefix_3': 'Wal',\n",
       "  'prefix_4': 'Wall',\n",
       "  'suffix_1': 'l',\n",
       "  'suffix_2': 'll',\n",
       "  'suffix_3': 'all',\n",
       "  'suffix_4': 'Wall',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 1,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'Wall',\n",
       "  'next_word': 'men',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'S',\n",
       "  'prefix_2': 'St',\n",
       "  'prefix_3': 'Str',\n",
       "  'prefix_4': 'Stre',\n",
       "  'suffix_1': 't',\n",
       "  'suffix_2': 'et',\n",
       "  'suffix_3': 'eet',\n",
       "  'suffix_4': 'reet',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'Street',\n",
       "  'next_word': 'and',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'm',\n",
       "  'prefix_2': 'me',\n",
       "  'prefix_3': 'men',\n",
       "  'prefix_4': 'men',\n",
       "  'suffix_1': 'n',\n",
       "  'suffix_2': 'en',\n",
       "  'suffix_3': 'men',\n",
       "  'suffix_4': 'men',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'men',\n",
       "  'next_word': 'women',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'a',\n",
       "  'prefix_2': 'an',\n",
       "  'prefix_3': 'and',\n",
       "  'prefix_4': 'and',\n",
       "  'suffix_1': 'd',\n",
       "  'suffix_2': 'nd',\n",
       "  'suffix_3': 'and',\n",
       "  'suffix_4': 'and',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'and',\n",
       "  'next_word': 'walk',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'w',\n",
       "  'prefix_2': 'wo',\n",
       "  'prefix_3': 'wom',\n",
       "  'prefix_4': 'wome',\n",
       "  'suffix_1': 'n',\n",
       "  'suffix_2': 'en',\n",
       "  'suffix_3': 'men',\n",
       "  'suffix_4': 'omen',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'women',\n",
       "  'next_word': 'with',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'w',\n",
       "  'prefix_2': 'wa',\n",
       "  'prefix_3': 'wal',\n",
       "  'prefix_4': 'walk',\n",
       "  'suffix_1': 'k',\n",
       "  'suffix_2': 'lk',\n",
       "  'suffix_3': 'alk',\n",
       "  'suffix_4': 'walk',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'walk',\n",
       "  'next_word': 'great',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'w',\n",
       "  'prefix_2': 'wi',\n",
       "  'prefix_3': 'wit',\n",
       "  'prefix_4': 'with',\n",
       "  'suffix_1': 'h',\n",
       "  'suffix_2': 'th',\n",
       "  'suffix_3': 'ith',\n",
       "  'suffix_4': 'with',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'with',\n",
       "  'next_word': 'purpose',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'g',\n",
       "  'prefix_2': 'gr',\n",
       "  'prefix_3': 'gre',\n",
       "  'prefix_4': 'grea',\n",
       "  'suffix_1': 't',\n",
       "  'suffix_2': 'at',\n",
       "  'suffix_3': 'eat',\n",
       "  'suffix_4': 'reat',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'great',\n",
       "  'next_word': ',',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'p',\n",
       "  'prefix_2': 'pu',\n",
       "  'prefix_3': 'pur',\n",
       "  'prefix_4': 'purp',\n",
       "  'suffix_1': 'e',\n",
       "  'suffix_2': 'se',\n",
       "  'suffix_3': 'ose',\n",
       "  'suffix_4': 'pose',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 1,\n",
       "  'prev_word': 'purpose',\n",
       "  'next_word': '*-2',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': ',',\n",
       "  'prefix_2': ',',\n",
       "  'prefix_3': ',',\n",
       "  'prefix_4': ',',\n",
       "  'suffix_1': ',',\n",
       "  'suffix_2': ',',\n",
       "  'suffix_3': ',',\n",
       "  'suffix_4': ',',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 1,\n",
       "  'prev_word': ',',\n",
       "  'next_word': 'noticing',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': '*',\n",
       "  'prefix_2': '*-',\n",
       "  'prefix_3': '*-2',\n",
       "  'prefix_4': '*-2',\n",
       "  'suffix_1': '2',\n",
       "  'suffix_2': '-2',\n",
       "  'suffix_3': '*-2',\n",
       "  'suffix_4': '*-2',\n",
       "  'word_has_hyphen': 1},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': '*-2',\n",
       "  'next_word': 'one',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'n',\n",
       "  'prefix_2': 'no',\n",
       "  'prefix_3': 'not',\n",
       "  'prefix_4': 'noti',\n",
       "  'suffix_1': 'g',\n",
       "  'suffix_2': 'ng',\n",
       "  'suffix_3': 'ing',\n",
       "  'suffix_4': 'cing',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'noticing',\n",
       "  'next_word': 'another',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'o',\n",
       "  'prefix_2': 'on',\n",
       "  'prefix_3': 'one',\n",
       "  'prefix_4': 'one',\n",
       "  'suffix_1': 'e',\n",
       "  'suffix_2': 'ne',\n",
       "  'suffix_3': 'one',\n",
       "  'suffix_4': 'one',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'one',\n",
       "  'next_word': 'only',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'a',\n",
       "  'prefix_2': 'an',\n",
       "  'prefix_3': 'ano',\n",
       "  'prefix_4': 'anot',\n",
       "  'suffix_1': 'r',\n",
       "  'suffix_2': 'er',\n",
       "  'suffix_3': 'her',\n",
       "  'suffix_4': 'ther',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'another',\n",
       "  'next_word': 'when',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'o',\n",
       "  'prefix_2': 'on',\n",
       "  'prefix_3': 'onl',\n",
       "  'prefix_4': 'only',\n",
       "  'suffix_1': 'y',\n",
       "  'suffix_2': 'ly',\n",
       "  'suffix_3': 'nly',\n",
       "  'suffix_4': 'only',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'only',\n",
       "  'next_word': 'they',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'w',\n",
       "  'prefix_2': 'wh',\n",
       "  'prefix_3': 'whe',\n",
       "  'prefix_4': 'when',\n",
       "  'suffix_1': 'n',\n",
       "  'suffix_2': 'en',\n",
       "  'suffix_3': 'hen',\n",
       "  'suffix_4': 'when',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'when',\n",
       "  'next_word': 'jostle',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 't',\n",
       "  'prefix_2': 'th',\n",
       "  'prefix_3': 'the',\n",
       "  'prefix_4': 'they',\n",
       "  'suffix_1': 'y',\n",
       "  'suffix_2': 'ey',\n",
       "  'suffix_3': 'hey',\n",
       "  'suffix_4': 'they',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'they',\n",
       "  'next_word': 'for',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'j',\n",
       "  'prefix_2': 'jo',\n",
       "  'prefix_3': 'jos',\n",
       "  'prefix_4': 'jost',\n",
       "  'suffix_1': 'e',\n",
       "  'suffix_2': 'le',\n",
       "  'suffix_3': 'tle',\n",
       "  'suffix_4': 'stle',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'jostle',\n",
       "  'next_word': 'cabs',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'f',\n",
       "  'prefix_2': 'fo',\n",
       "  'prefix_3': 'for',\n",
       "  'prefix_4': 'for',\n",
       "  'suffix_1': 'r',\n",
       "  'suffix_2': 'or',\n",
       "  'suffix_3': 'for',\n",
       "  'suffix_4': 'for',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 0,\n",
       "  'prev_word': 'for',\n",
       "  'next_word': '*T*-1',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': 'c',\n",
       "  'prefix_2': 'ca',\n",
       "  'prefix_3': 'cab',\n",
       "  'prefix_4': 'cabs',\n",
       "  'suffix_1': 's',\n",
       "  'suffix_2': 'bs',\n",
       "  'suffix_3': 'abs',\n",
       "  'suffix_4': 'cabs',\n",
       "  'word_has_hyphen': 0},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 0,\n",
       "  'is_complete_capital': 1,\n",
       "  'prev_word': 'cabs',\n",
       "  'next_word': '.',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 1,\n",
       "  'prefix_1': '*',\n",
       "  'prefix_2': '*T',\n",
       "  'prefix_3': '*T*',\n",
       "  'prefix_4': '*T*-',\n",
       "  'suffix_1': '1',\n",
       "  'suffix_2': '-1',\n",
       "  'suffix_3': '*-1',\n",
       "  'suffix_4': 'T*-1',\n",
       "  'word_has_hyphen': 1},\n",
       " {'is_first_capital': 0,\n",
       "  'is_first_word': 0,\n",
       "  'is_last_word': 1,\n",
       "  'is_complete_capital': 1,\n",
       "  'prev_word': '*T*-1',\n",
       "  'next_word': '',\n",
       "  'is_numeric': 0,\n",
       "  'is_alphanumeric': 0,\n",
       "  'prefix_1': '.',\n",
       "  'prefix_2': '.',\n",
       "  'prefix_3': '.',\n",
       "  'prefix_4': '.',\n",
       "  'suffix_1': '.',\n",
       "  'suffix_2': '.',\n",
       "  'suffix_3': '.',\n",
       "  'suffix_4': '.',\n",
       "  'word_has_hyphen': 0}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = prepareData(train_set)\n",
    "X_test, y_test = prepareData(test_set)\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADP',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'CONJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " '.',\n",
       " 'X',\n",
       " 'VERB',\n",
       " 'NUM',\n",
       " 'DET',\n",
       " 'ADV',\n",
       " 'ADV',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'X',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.01,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "try:\n",
    "    crf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "predictions = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9738471726864286"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.flat_f1_score(y_test, predictions, average='weighted', labels=crf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9963402924209424"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_train = crf.predict(X_train)\n",
    "metrics.flat_f1_score(y_train, predictions_train, average='weighted', labels=crf.classes_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref https://nlpforhackers.io/lstm-pos-tagger-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "Tagged sentences:  3914\n",
      "Tagged words:  100676\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "print(tagged_sentences[0])\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words: \", len(nltk.corpus.treebank.tagged_words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorillard' 'Inc.' ',' 'the' 'unit' 'of' 'New' 'York-based' 'Loews'\n",
      " 'Corp.' 'that' '*T*-2' 'makes' 'Kent' 'cigarettes' ',' 'stopped' 'using'\n",
      " 'crocidolite' 'in' 'its' 'Micronite' 'cigarette' 'filters' 'in' '1956'\n",
      " '.']\n",
      "['NNP' 'NNP' ',' 'DT' 'NN' 'IN' 'JJ' 'JJ' 'NNP' 'NNP' 'WDT' '-NONE-' 'VBZ'\n",
      " 'NNP' 'NNS' ',' 'VBD' 'VBG' 'NN' 'IN' 'PRP$' 'NN' 'NN' 'NNS' 'IN' 'CD'\n",
      " '.']\n"
     ]
    }
   ],
   "source": [
    "sentences, sentence_tags = [], []\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append(np.array(sentence))\n",
    "    sentence_tags.append(np.array(tags))\n",
    "\n",
    "# Let's see how a sequence looks\n",
    "\n",
    "print(sentences[5])\n",
    "print(sentence_tags[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(train_sentences,\n",
    "test_sentences,\n",
    "train_tags,\n",
    "test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras also needs to work with numbers,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    "\n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "    \n",
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    "\n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0\n",
    "word2index['-OOV-'] = 1\n",
    "\n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[969, 2365, 3824, 1989, 4376, 1673, 2784, 6893, 1943, 4376, 2432, 3300, 4244, 2712, 3438, 1032, 7228, 147, 4138, 6805, 9700, 3263, 3300, 1771, 5768, 4673, 6824, 644, 1964, 9837, 5768, 9391, 5888, 6272, 6721, 4140, 228, 858, 9636, 2628, 1976]\n",
      "[8682, 5171, 8843, 342, 9776, 9411, 2550, 3438, 107, 6272, 4376, 7468, 4760, 1, 9967, 3300, 4376, 1346, 8755, 4123, 1976]\n",
      "[21, 34, 23, 32, 42, 39, 11, 28, 1, 42, 33, 29, 17, 41, 30, 42, 40, 41, 5, 34, 3, 32, 29, 39, 12, 18, 11, 32, 33, 33, 12, 39, 39, 5, 4, 20, 5, 33, 33, 33, 8]\n",
      "[33, 33, 33, 24, 17, 28, 32, 30, 20, 5, 42, 4, 39, 28, 32, 29, 42, 39, 46, 20, 8]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    "\n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    \n",
    "    train_sentences_X.append(s_int)\n",
    "\n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    \n",
    "    test_sentences_X.append(s_int)\n",
    "\n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    "\n",
    "for s in test_tags:\n",
    "    test_tags_y.append([tag2index[t] for t in s])\n",
    "\n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "MAX_LEGTH = len(max(train_sentences_X, key=len))\n",
    "print(MAX_LEGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 969 2365 3824 1989 4376 1673 2784 6893 1943 4376 2432 3300 4244 2712\n",
      " 3438 1032 7228  147 4138 6805 9700 3263 3300 1771 5768 4673 6824  644\n",
      " 1964 9837 5768 9391 5888 6272 6721 4140  228  858 9636 2628 1976    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      "[8682 5171 8843  342 9776 9411 2550 3438  107 6272 4376 7468 4760    1\n",
      " 9967 3300 4376 1346 8755 4123 1976    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      "[21 34 23 32 42 39 11 28  1 42 33 29 17 41 30 42 40 41  5 34  3 32 29 39\n",
      " 12 18 11 32 33 33 12 39 39  5  4 20  5 33 33 33  8  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n",
      "[33 33 33 24 17 28 32 30 20  5 42  4 39 28 32 29 42 39 46 20  8  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LEGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LEGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LEGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LEGTH, padding='post')\n",
    "\n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 271, 128)          1298176   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 271, 512)         788480    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 271, 47)          24111     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 271, 47)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,110,767\n",
      "Trainable params: 2,110,767\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import adam_v2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LEGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam_v2.Adam(0.001), metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "20/20 [==============================] - 14s 288ms/step - loss: 1.2044 - accuracy: 0.8591 - val_loss: 0.3659 - val_accuracy: 0.9058\n",
      "Epoch 2/40\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 0.3325 - accuracy: 0.9055 - val_loss: 0.3208 - val_accuracy: 0.9052\n",
      "Epoch 3/40\n",
      "20/20 [==============================] - 4s 213ms/step - loss: 0.3134 - accuracy: 0.9095 - val_loss: 0.3087 - val_accuracy: 0.9163\n",
      "Epoch 4/40\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 0.3030 - accuracy: 0.9170 - val_loss: 0.3028 - val_accuracy: 0.9172\n",
      "Epoch 5/40\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.2953 - accuracy: 0.9174 - val_loss: 0.2963 - val_accuracy: 0.9173\n",
      "Epoch 6/40\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.2882 - accuracy: 0.9175 - val_loss: 0.2882 - val_accuracy: 0.9174\n",
      "Epoch 7/40\n",
      "20/20 [==============================] - 4s 209ms/step - loss: 0.2807 - accuracy: 0.9180 - val_loss: 0.2809 - val_accuracy: 0.9201\n",
      "Epoch 8/40\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 0.2741 - accuracy: 0.9217 - val_loss: 0.2750 - val_accuracy: 0.9248\n",
      "Epoch 9/40\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 0.2685 - accuracy: 0.9259 - val_loss: 0.2692 - val_accuracy: 0.9304\n",
      "Epoch 10/40\n",
      "20/20 [==============================] - 4s 213ms/step - loss: 0.2622 - accuracy: 0.9326 - val_loss: 0.2622 - val_accuracy: 0.9358\n",
      "Epoch 11/40\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.2544 - accuracy: 0.9389 - val_loss: 0.2516 - val_accuracy: 0.9412\n",
      "Epoch 12/40\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.2416 - accuracy: 0.9439 - val_loss: 0.2361 - val_accuracy: 0.9452\n",
      "Epoch 13/40\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.2238 - accuracy: 0.9487 - val_loss: 0.2169 - val_accuracy: 0.9506\n",
      "Epoch 14/40\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.2030 - accuracy: 0.9512 - val_loss: 0.1963 - val_accuracy: 0.9522\n",
      "Epoch 15/40\n",
      "20/20 [==============================] - 4s 214ms/step - loss: 0.1822 - accuracy: 0.9542 - val_loss: 0.1768 - val_accuracy: 0.9537\n",
      "Epoch 16/40\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.1624 - accuracy: 0.9571 - val_loss: 0.1590 - val_accuracy: 0.9588\n",
      "Epoch 17/40\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 0.1430 - accuracy: 0.9618 - val_loss: 0.1407 - val_accuracy: 0.9624\n",
      "Epoch 18/40\n",
      "20/20 [==============================] - 4s 214ms/step - loss: 0.1226 - accuracy: 0.9679 - val_loss: 0.1221 - val_accuracy: 0.9689\n",
      "Epoch 19/40\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 0.1023 - accuracy: 0.9750 - val_loss: 0.1049 - val_accuracy: 0.9739\n",
      "Epoch 20/40\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 0.0831 - accuracy: 0.9814 - val_loss: 0.0890 - val_accuracy: 0.9801\n",
      "Epoch 21/40\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.0662 - accuracy: 0.9866 - val_loss: 0.0759 - val_accuracy: 0.9832\n",
      "Epoch 22/40\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 0.0523 - accuracy: 0.9898 - val_loss: 0.0654 - val_accuracy: 0.9856\n",
      "Epoch 23/40\n",
      "20/20 [==============================] - 4s 214ms/step - loss: 0.0417 - accuracy: 0.9920 - val_loss: 0.0576 - val_accuracy: 0.9870\n",
      "Epoch 24/40\n",
      "20/20 [==============================] - 4s 213ms/step - loss: 0.0338 - accuracy: 0.9935 - val_loss: 0.0519 - val_accuracy: 0.9881\n",
      "Epoch 25/40\n",
      "20/20 [==============================] - 5s 238ms/step - loss: 0.0279 - accuracy: 0.9945 - val_loss: 0.0479 - val_accuracy: 0.9886\n",
      "Epoch 26/40\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 0.0237 - accuracy: 0.9953 - val_loss: 0.0450 - val_accuracy: 0.9891\n",
      "Epoch 27/40\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 0.0205 - accuracy: 0.9958 - val_loss: 0.0430 - val_accuracy: 0.9896\n",
      "Epoch 28/40\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 0.0179 - accuracy: 0.9963 - val_loss: 0.0413 - val_accuracy: 0.9899\n",
      "Epoch 29/40\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.0159 - accuracy: 0.9967 - val_loss: 0.0401 - val_accuracy: 0.9901\n",
      "Epoch 30/40\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 0.0143 - accuracy: 0.9971 - val_loss: 0.0392 - val_accuracy: 0.9903\n",
      "Epoch 31/40\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 0.0129 - accuracy: 0.9973 - val_loss: 0.0382 - val_accuracy: 0.9905\n",
      "Epoch 32/40\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.0117 - accuracy: 0.9975 - val_loss: 0.0374 - val_accuracy: 0.9907\n",
      "Epoch 33/40\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.0107 - accuracy: 0.9978 - val_loss: 0.0369 - val_accuracy: 0.9909\n",
      "Epoch 34/40\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.0099 - accuracy: 0.9979 - val_loss: 0.0366 - val_accuracy: 0.9907\n",
      "Epoch 35/40\n",
      "20/20 [==============================] - 4s 213ms/step - loss: 0.0092 - accuracy: 0.9981 - val_loss: 0.0362 - val_accuracy: 0.9909\n",
      "Epoch 36/40\n",
      "20/20 [==============================] - 5s 245ms/step - loss: 0.0085 - accuracy: 0.9983 - val_loss: 0.0365 - val_accuracy: 0.9911\n",
      "Epoch 37/40\n",
      "20/20 [==============================] - 4s 218ms/step - loss: 0.0079 - accuracy: 0.9984 - val_loss: 0.0358 - val_accuracy: 0.9909\n",
      "Epoch 38/40\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0356 - val_accuracy: 0.9911\n",
      "Epoch 39/40\n",
      "20/20 [==============================] - 4s 209ms/step - loss: 0.0069 - accuracy: 0.9986 - val_loss: 0.0358 - val_accuracy: 0.9912\n",
      "Epoch 40/40\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 0.0064 - accuracy: 0.9987 - val_loss: 0.0354 - val_accuracy: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1828efc2590>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0335 - accuracy: 0.9913\n",
      "accuracy : False\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]} : {scores[1] == 100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd6d6cb290673ab90f7eabfbb5e0872757dd79fe611a96b9ca3462dfc1ba1bf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
